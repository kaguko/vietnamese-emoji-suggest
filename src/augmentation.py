"""
Data augmentation and weak labeling module for Vietnamese Emoji Suggestion System.

This module provides:
- Synonym replacement augmentation
- Weak labeling using rule-based methods
- Data validation and quality control
"""

import re
import random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import json

# Vietnamese synonym dictionary for augmentation
VIETNAMESE_SYNONYMS = {
    # Joy synonyms
    "vui": ["h·∫°nh ph√∫c", "sung s∆∞·ªõng", "ph·∫•n kh·ªüi", "h√†o h·ª©ng"],
    "h·∫°nh ph√∫c": ["vui", "sung s∆∞·ªõng", "m√£n nguy·ªán"],
    "tuy·ªát v·ªùi": ["tuy·ªát", "hay", "xu·∫•t s·∫Øc", "t·ªët l·∫Øm"],
    "hay": ["t·ªët", "tuy·ªát", "gi·ªèi", "ƒë·ªânh"],
    "th√≠ch": ["y√™u th√≠ch", "∆∞a", "m√™"],
    
    # Sadness synonyms
    "bu·ªìn": ["ƒëau bu·ªìn", "u s·∫ßu", "ch√°n n·∫£n", "th·∫•t v·ªçng"],
    "ƒëau": ["ƒëau ƒë·ªõn", "kh·ªï s·ªü", "x√≥t xa"],
    "nh·ªõ": ["th∆∞∆°ng nh·ªõ", "nhung nh·ªõ", "da di·∫øt"],
    "ch√°n": ["bu·ªìn ch√°n", "t·∫ª nh·∫°t", "nh√†m"],
    
    # Anger synonyms
    "gi·∫≠n": ["t·ª©c gi·∫≠n", "n·ªïi gi·∫≠n", "b·ª±c t·ª©c"],
    "t·ª©c": ["b·ª±c m√¨nh", "kh√≥ ch·ªãu", "gi·∫≠n d·ªØ"],
    "gh√©t": ["cƒÉm gh√©t", "gh√™ t·ªüm", "ch√°n gh√©t"],
    
    # Fear synonyms
    "s·ª£": ["lo s·ª£", "kinh s·ª£", "ho·∫£ng s·ª£"],
    "lo": ["lo l·∫Øng", "lo √¢u", "b·ªìn ch·ªìn"],
    "cƒÉng th·∫≥ng": ["√°p l·ª±c", "stress", "h·ªìi h·ªôp"],
    
    # General
    "r·∫•t": ["c·ª±c k·ª≥", "v√¥ c√πng", "qu√°", "si√™u"],
    "qu√°": ["l·∫Øm", "gh√™", "c·ª±c", "th·∫≠t"],
}

# Emotion keywords for weak labeling
EMOTION_KEYWORDS = {
    "joy": {
        "strong": ["tuy·ªát v·ªùi", "h·∫°nh ph√∫c", "sung s∆∞·ªõng", "y√™u", "th√≠ch qu√°", 
                   "ch√∫c m·ª´ng", "t·ªët qu√°", "hay qu√°", "xu·∫•t s·∫Øc"],
        "medium": ["vui", "t·ªët", "hay", "ƒë∆∞·ª£c", "ok", "·ªïn", "th√≠ch"],
        "weak": ["c≈©ng ƒë∆∞·ª£c", "t·∫°m", "b√¨nh th∆∞·ªùng"]
    },
    "sadness": {
        "strong": ["ƒëau kh·ªï", "kh√≥c", "th·∫•t v·ªçng qu√°", "bu·ªìn qu√°", "ch√°n qu√°"],
        "medium": ["bu·ªìn", "nh·ªõ", "ti·∫øc", "ƒëau", "th∆∞∆°ng"],
        "weak": ["h∆°i bu·ªìn", "ch√°n", "m·ªát"]
    },
    "anger": {
        "strong": ["ƒëi√™n ti·∫øt", "gi·∫≠n d·ªØ", "t·ª©c ch·∫øt", "gh√©t cay"],
        "medium": ["t·ª©c", "gi·∫≠n", "b·ª±c", "kh√≥ ch·ªãu", "gh√©t"],
        "weak": ["h∆°i t·ª©c", "kh√≥ ch·ªãu", "b·ª±c m√¨nh"]
    },
    "fear": {
        "strong": ["kinh ho√†ng", "ho·∫£ng lo·∫°n", "s·ª£ ch·∫øt", "run r·∫©y"],
        "medium": ["s·ª£", "lo l·∫Øng", "cƒÉng th·∫≥ng", "h·ªìi h·ªôp"],
        "weak": ["h∆°i lo", "h∆°i s·ª£", "e ng·∫°i"]
    },
    "surprise": {
        "strong": ["s·ªëc", "kh√¥ng tin n·ªïi", "tr·ªùi ∆°i"],
        "medium": ["ng·∫°c nhi√™n", "b·∫•t ng·ªù", "wow", "·ªßa"],
        "weak": ["h∆°i ng·∫°c nhi√™n", "l·∫°"]
    },
    "disgust": {
        "strong": ["gh√™ t·ªüm", "kinh t·ªüm", "bu·ªìn n√¥n"],
        "medium": ["gh√™", "kinh", "d∆°", "b·∫©n"],
        "weak": ["h∆°i gh√™", "k·ª≥"]
    },
    "trust": {
        "strong": ["tin t∆∞·ªüng tuy·ªát ƒë·ªëi", "ch·∫Øc ch·∫Øn"],
        "medium": ["tin", "·ªßng h·ªô", "y√™n t√¢m"],
        "weak": ["c√≥ l·∫Ω", "ƒë∆∞·ª£c"]
    },
    "anticipation": {
        "strong": ["h√°o h·ª©c qu√°", "kh√¥ng ch·ªù ƒë∆∞·ª£c"],
        "medium": ["mong", "ch·ªù ƒë·ª£i", "hy v·ªçng"],
        "weak": ["h∆°i mong", "ƒë·ª£i"]
    }
}


@dataclass
class AugmentedSample:
    """Augmented data sample with metadata."""
    original_text: str
    augmented_text: str
    augmentation_type: str
    primary_emotion: str
    intensity: int
    emoji_1: str
    emoji_2: Optional[str] = None
    emoji_3: Optional[str] = None
    confidence: float = 1.0  # 1.0 for manual, < 1.0 for weak-labeled


def synonym_replacement(text: str, n_replacements: int = 1) -> str:
    """
    Replace n words with synonyms.
    
    Args:
        text: Input text
        n_replacements: Number of words to replace
        
    Returns:
        Augmented text
    """
    words = text.split()
    new_words = words.copy()
    
    # Find replaceable words
    replaceable_indices = []
    for i, word in enumerate(words):
        word_lower = word.lower()
        if word_lower in VIETNAMESE_SYNONYMS:
            replaceable_indices.append(i)
    
    if not replaceable_indices:
        return text
    
    # Randomly select words to replace
    n_to_replace = min(n_replacements, len(replaceable_indices))
    indices_to_replace = random.sample(replaceable_indices, n_to_replace)
    
    for idx in indices_to_replace:
        word = words[idx].lower()
        synonyms = VIETNAMESE_SYNONYMS.get(word, [])
        if synonyms:
            new_words[idx] = random.choice(synonyms)
    
    return " ".join(new_words)


def intensity_variation(text: str, current_intensity: int) -> Tuple[str, int]:
    """
    Create variations by adding/removing intensity modifiers.
    
    Args:
        text: Input text
        current_intensity: Current intensity level (1-5)
        
    Returns:
        Tuple of (modified_text, new_intensity)
    """
    intensifiers = ["r·∫•t", "c·ª±c k·ª≥", "v√¥ c√πng", "qu√°", "si√™u"]
    weakeners = ["h∆°i", "m·ªôt ch√∫t", "t·∫°m"]
    
    text_lower = text.lower()
    
    # Try to increase intensity
    if current_intensity < 5:
        for intensifier in intensifiers:
            if intensifier not in text_lower:
                # Add intensifier at beginning or before adjective
                words = text.split()
                if len(words) > 1:
                    words.insert(1, intensifier)
                    return " ".join(words), min(5, current_intensity + 1)
    
    # Try to decrease intensity
    if current_intensity > 1:
        for intensifier in intensifiers:
            if intensifier in text_lower:
                new_text = text_lower.replace(intensifier, "").strip()
                new_text = re.sub(r'\s+', ' ', new_text)
                return new_text, max(1, current_intensity - 1)
    
    return text, current_intensity


def weak_label_text(text: str) -> Tuple[Optional[str], int, float]:
    """
    Automatically label text using keyword matching (weak labeling).
    
    Args:
        text: Input text
        
    Returns:
        Tuple of (emotion, intensity, confidence)
        Returns (None, 0, 0) if no emotion detected
    """
    text_lower = text.lower()
    
    best_emotion = None
    best_intensity = 0
    best_confidence = 0.0
    
    for emotion, intensity_keywords in EMOTION_KEYWORDS.items():
        # Check strong keywords first
        for keyword in intensity_keywords["strong"]:
            if keyword in text_lower:
                if best_confidence < 0.9:
                    best_emotion = emotion
                    best_intensity = 5
                    best_confidence = 0.9
                break
        
        # Check medium keywords
        for keyword in intensity_keywords["medium"]:
            if keyword in text_lower:
                if best_confidence < 0.7:
                    best_emotion = emotion
                    best_intensity = 3
                    best_confidence = 0.7
                break
        
        # Check weak keywords
        for keyword in intensity_keywords["weak"]:
            if keyword in text_lower:
                if best_confidence < 0.5:
                    best_emotion = emotion
                    best_intensity = 2
                    best_confidence = 0.5
                break
    
    return best_emotion, best_intensity, best_confidence


def augment_dataset(
    samples: List[Dict],
    augmentation_factor: int = 2,
    include_weak_labeled: bool = True
) -> List[Dict]:
    """
    Augment dataset with synonym replacement and intensity variations.
    
    Args:
        samples: Original samples
        augmentation_factor: How many augmented samples per original
        include_weak_labeled: Whether to include weak-labeled samples
        
    Returns:
        Augmented dataset
    """
    augmented = []
    
    for sample in samples:
        # Keep original
        sample['confidence'] = 1.0
        sample['augmentation_type'] = 'original'
        augmented.append(sample.copy())
        
        text = sample['text']
        emotion = sample['primary_emotion']
        intensity = sample['intensity']
        
        # Synonym replacement
        for i in range(augmentation_factor):
            aug_text = synonym_replacement(text, n_replacements=1)
            if aug_text != text:
                aug_sample = sample.copy()
                aug_sample['text'] = aug_text
                aug_sample['confidence'] = 0.95
                aug_sample['augmentation_type'] = 'synonym'
                augmented.append(aug_sample)
        
        # Intensity variation (only if intensity can change)
        if 2 <= intensity <= 4:
            var_text, var_intensity = intensity_variation(text, intensity)
            if var_text != text:
                var_sample = sample.copy()
                var_sample['text'] = var_text
                var_sample['intensity'] = var_intensity
                var_sample['confidence'] = 0.9
                var_sample['augmentation_type'] = 'intensity'
                augmented.append(var_sample)
    
    return augmented


def generate_weak_labeled_samples(
    seed_texts: List[str],
    emoji_map: Dict[str, List[str]]
) -> List[Dict]:
    """
    Generate weak-labeled samples from unlabeled texts.
    
    Args:
        seed_texts: List of unlabeled Vietnamese texts
        emoji_map: Emotion to emoji mapping
        
    Returns:
        List of weak-labeled samples
    """
    samples = []
    
    for text in seed_texts:
        emotion, intensity, confidence = weak_label_text(text)
        
        if emotion and confidence >= 0.5:
            emojis = emoji_map.get(emotion, ["ü§î", "üòä", "üëç"])
            
            sample = {
                'text': text,
                'primary_emotion': emotion,
                'intensity': intensity,
                'emoji_1': emojis[0] if len(emojis) > 0 else "ü§î",
                'emoji_2': emojis[1] if len(emojis) > 1 else None,
                'emoji_3': emojis[2] if len(emojis) > 2 else None,
                'confidence': confidence,
                'augmentation_type': 'weak_labeled',
                'source': 'auto'
            }
            samples.append(sample)
    
    return samples


def validate_dataset(samples: List[Dict], min_confidence: float = 0.5) -> Dict:
    """
    Validate dataset quality.
    
    Args:
        samples: Dataset samples
        min_confidence: Minimum confidence threshold
        
    Returns:
        Validation report
    """
    total = len(samples)
    high_quality = sum(1 for s in samples if s.get('confidence', 1.0) >= 0.9)
    medium_quality = sum(1 for s in samples if 0.7 <= s.get('confidence', 1.0) < 0.9)
    low_quality = sum(1 for s in samples if s.get('confidence', 1.0) < 0.7)
    
    # Emotion distribution
    emotion_counts = {}
    for s in samples:
        emotion = s.get('primary_emotion', 'unknown')
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
    
    # Check balance
    avg_per_emotion = total / len(emotion_counts) if emotion_counts else 0
    imbalance_ratio = max(emotion_counts.values()) / min(emotion_counts.values()) if emotion_counts else 0
    
    return {
        'total_samples': total,
        'high_quality': high_quality,
        'medium_quality': medium_quality,
        'low_quality': low_quality,
        'emotion_distribution': emotion_counts,
        'avg_per_emotion': avg_per_emotion,
        'imbalance_ratio': imbalance_ratio,
        'is_balanced': imbalance_ratio < 2.0,
        'quality_score': (high_quality * 1.0 + medium_quality * 0.7 + low_quality * 0.4) / total if total > 0 else 0
    }


# Sample unlabeled texts for weak labeling
SAMPLE_UNLABELED_TEXTS = [
    "H√¥m nay tr·ªùi ƒë·∫πp qu√°",
    "C√¥ng vi·ªác √°p l·ª±c qu√°",
    "ƒê∆∞·ª£c tƒÉng l∆∞∆°ng r·ªìi",
    "B·ªã s·∫øp m·∫Øng",
    "Mai thi r·ªìi lo qu√°",
    "Tin ƒë∆∞·ª£c kh√¥ng",
    "D·ªçn ph√≤ng m·ªát gh√™",
    "Cu·ªëi tu·∫ßn ƒëi ch∆°i",
    "Deadline g·∫•p qu√°",
    "Team m√¨nh th·∫Øng r·ªìi",
    "B·∫°n b√® xa d·∫ßn",
    "Th·ª©c khuya qu√° m·ªát",
    "ƒÇn ngon qu√°",
    "Phim hay gh√™",
    "Ch·ªù k·∫øt qu·∫£ h·ªìi h·ªôp",
    "ƒê∆∞·ªùng t·∫Øc kinh kh·ªßng",
    "ƒê∆∞·ª£c ngh·ªâ ph√©p vui qu√°",
    "B·ªã cancel k·∫ø ho·∫°ch",
    "H·ªçc kh√¥ng hi·ªÉu g√¨",
    "G·∫∑p l·∫°i b·∫°n c≈©",
]


if __name__ == "__main__":
    from data.collect_data import create_initial_dataset, save_dataset_csv
    from src.models import EMOTION_EMOJI_MAP
    
    print("=" * 60)
    print("DATA AUGMENTATION & WEAK LABELING")
    print("=" * 60)
    
    # 1. Load original dataset
    original_samples = create_initial_dataset()
    print(f"\n1. Original samples: {len(original_samples)}")
    
    # 2. Augment with synonyms
    augmented_samples = augment_dataset(original_samples, augmentation_factor=2)
    print(f"2. After augmentation: {len(augmented_samples)}")
    
    # 3. Generate weak-labeled samples
    weak_labeled = generate_weak_labeled_samples(SAMPLE_UNLABELED_TEXTS, EMOTION_EMOJI_MAP)
    print(f"3. Weak-labeled samples: {len(weak_labeled)}")
    
    # 4. Combine
    all_samples = augmented_samples + weak_labeled
    print(f"4. Total samples: {len(all_samples)}")
    
    # 5. Validate
    validation = validate_dataset(all_samples)
    print(f"\n5. Validation Report:")
    print(f"   - High quality: {validation['high_quality']}")
    print(f"   - Medium quality: {validation['medium_quality']}")
    print(f"   - Low quality: {validation['low_quality']}")
    print(f"   - Quality score: {validation['quality_score']:.2%}")
    print(f"   - Balanced: {validation['is_balanced']}")
    
    # 6. Save
    save_dataset_csv(all_samples, "data/processed/augmented_data.csv")
    print(f"\n‚úì Saved to data/processed/augmented_data.csv")
