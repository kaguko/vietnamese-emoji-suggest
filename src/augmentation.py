"""
Data augmentation and weak labeling module for Vietnamese Emoji Suggestion System.

This module provides:
- Synonym replacement augmentation
- Weak labeling using rule-based methods
- Data validation and quality control
"""

import re
import random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import json

# Vietnamese synonym dictionary for augmentation
VIETNAMESE_SYNONYMS = {
    # Joy synonyms
    "vui": ["h·∫°nh ph√∫c", "sung s∆∞·ªõng", "ph·∫•n kh·ªüi", "h√†o h·ª©ng"],
    "h·∫°nh ph√∫c": ["vui", "sung s∆∞·ªõng", "m√£n nguy·ªán"],
    "tuy·ªát v·ªùi": ["tuy·ªát", "hay", "xu·∫•t s·∫Øc", "t·ªët l·∫Øm"],
    "hay": ["t·ªët", "tuy·ªát", "gi·ªèi", "ƒë·ªânh"],
    "th√≠ch": ["y√™u th√≠ch", "∆∞a", "m√™"],
    
    # Sadness synonyms
    "bu·ªìn": ["ƒëau bu·ªìn", "u s·∫ßu", "ch√°n n·∫£n", "th·∫•t v·ªçng"],
    "ƒëau": ["ƒëau ƒë·ªõn", "kh·ªï s·ªü", "x√≥t xa"],
    "nh·ªõ": ["th∆∞∆°ng nh·ªõ", "nhung nh·ªõ", "da di·∫øt"],
    "ch√°n": ["bu·ªìn ch√°n", "t·∫ª nh·∫°t", "nh√†m"],
    
    # Anger synonyms
    "gi·∫≠n": ["t·ª©c gi·∫≠n", "n·ªïi gi·∫≠n", "b·ª±c t·ª©c"],
    "t·ª©c": ["b·ª±c m√¨nh", "kh√≥ ch·ªãu", "gi·∫≠n d·ªØ"],
    "gh√©t": ["cƒÉm gh√©t", "gh√™ t·ªüm", "ch√°n gh√©t"],
    
    # Fear synonyms
    "s·ª£": ["lo s·ª£", "kinh s·ª£", "ho·∫£ng s·ª£"],
    "lo": ["lo l·∫Øng", "lo √¢u", "b·ªìn ch·ªìn"],
    "cƒÉng th·∫≥ng": ["√°p l·ª±c", "stress", "h·ªìi h·ªôp"],
    
    # General
    "r·∫•t": ["c·ª±c k·ª≥", "v√¥ c√πng", "qu√°", "si√™u"],
    "qu√°": ["l·∫Øm", "gh√™", "c·ª±c", "th·∫≠t"],
}

# Emotion keywords for weak labeling
EMOTION_KEYWORDS = {
    "joy": {
        "strong": ["tuy·ªát v·ªùi", "h·∫°nh ph√∫c", "sung s∆∞·ªõng", "y√™u", "th√≠ch qu√°", 
                   "ch√∫c m·ª´ng", "t·ªët qu√°", "hay qu√°", "xu·∫•t s·∫Øc"],
        "medium": ["vui", "t·ªët", "hay", "ƒë∆∞·ª£c", "ok", "·ªïn", "th√≠ch"],
        "weak": ["c≈©ng ƒë∆∞·ª£c", "t·∫°m", "b√¨nh th∆∞·ªùng"]
    },
    "sadness": {
        "strong": ["ƒëau kh·ªï", "kh√≥c", "th·∫•t v·ªçng qu√°", "bu·ªìn qu√°", "ch√°n qu√°"],
        "medium": ["bu·ªìn", "nh·ªõ", "ti·∫øc", "ƒëau", "th∆∞∆°ng"],
        "weak": ["h∆°i bu·ªìn", "ch√°n", "m·ªát"]
    },
    "anger": {
        "strong": ["ƒëi√™n ti·∫øt", "gi·∫≠n d·ªØ", "t·ª©c ch·∫øt", "gh√©t cay"],
        "medium": ["t·ª©c", "gi·∫≠n", "b·ª±c", "kh√≥ ch·ªãu", "gh√©t"],
        "weak": ["h∆°i t·ª©c", "kh√≥ ch·ªãu", "b·ª±c m√¨nh"]
    },
    "fear": {
        "strong": ["kinh ho√†ng", "ho·∫£ng lo·∫°n", "s·ª£ ch·∫øt", "run r·∫©y"],
        "medium": ["s·ª£", "lo l·∫Øng", "cƒÉng th·∫≥ng", "h·ªìi h·ªôp"],
        "weak": ["h∆°i lo", "h∆°i s·ª£", "e ng·∫°i"]
    },
    "surprise": {
        "strong": ["s·ªëc", "kh√¥ng tin n·ªïi", "tr·ªùi ∆°i"],
        "medium": ["ng·∫°c nhi√™n", "b·∫•t ng·ªù", "wow", "·ªßa"],
        "weak": ["h∆°i ng·∫°c nhi√™n", "l·∫°"]
    },
    "disgust": {
        "strong": ["gh√™ t·ªüm", "kinh t·ªüm", "bu·ªìn n√¥n"],
        "medium": ["gh√™", "kinh", "d∆°", "b·∫©n"],
        "weak": ["h∆°i gh√™", "k·ª≥"]
    },
    "trust": {
        "strong": ["tin t∆∞·ªüng tuy·ªát ƒë·ªëi", "ch·∫Øc ch·∫Øn"],
        "medium": ["tin", "·ªßng h·ªô", "y√™n t√¢m"],
        "weak": ["c√≥ l·∫Ω", "ƒë∆∞·ª£c"]
    },
    "anticipation": {
        "strong": ["h√°o h·ª©c qu√°", "kh√¥ng ch·ªù ƒë∆∞·ª£c"],
        "medium": ["mong", "ch·ªù ƒë·ª£i", "hy v·ªçng"],
        "weak": ["h∆°i mong", "ƒë·ª£i"]
    }
}


@dataclass
class AugmentedSample:
    """Augmented data sample with metadata."""
    original_text: str
    augmented_text: str
    augmentation_type: str
    primary_emotion: str
    intensity: int
    emoji_1: str
    emoji_2: Optional[str] = None
    emoji_3: Optional[str] = None
    confidence: float = 1.0  # 1.0 for manual, < 1.0 for weak-labeled


def synonym_replacement(text: str, n_replacements: int = 1) -> str:
    """
    Replace n words with synonyms.
    
    Args:
        text: Input text
        n_replacements: Number of words to replace
        
    Returns:
        Augmented text
    """
    words = text.split()
    new_words = words.copy()
    
    # Find replaceable words
    replaceable_indices = []
    for i, word in enumerate(words):
        word_lower = word.lower()
        if word_lower in VIETNAMESE_SYNONYMS:
            replaceable_indices.append(i)
    
    if not replaceable_indices:
        return text
    
    # Randomly select words to replace
    n_to_replace = min(n_replacements, len(replaceable_indices))
    indices_to_replace = random.sample(replaceable_indices, n_to_replace)
    
    for idx in indices_to_replace:
        word = words[idx].lower()
        synonyms = VIETNAMESE_SYNONYMS.get(word, [])
        if synonyms:
            new_words[idx] = random.choice(synonyms)
    
    return " ".join(new_words)


def intensity_variation(text: str, current_intensity: int) -> Tuple[str, int]:
    """
    Create variations by adding/removing intensity modifiers.
    
    Args:
        text: Input text
        current_intensity: Current intensity level (1-5)
        
    Returns:
        Tuple of (modified_text, new_intensity)
    """
    intensifiers = ["r·∫•t", "c·ª±c k·ª≥", "v√¥ c√πng", "qu√°", "si√™u"]
    weakeners = ["h∆°i", "m·ªôt ch√∫t", "t·∫°m"]
    
    text_lower = text.lower()
    
    # Try to increase intensity
    if current_intensity < 5:
        for intensifier in intensifiers:
            if intensifier not in text_lower:
                # Add intensifier at beginning or before adjective
                words = text.split()
                if len(words) > 1:
                    words.insert(1, intensifier)
                    return " ".join(words), min(5, current_intensity + 1)
    
    # Try to decrease intensity
    if current_intensity > 1:
        for intensifier in intensifiers:
            if intensifier in text_lower:
                new_text = text_lower.replace(intensifier, "").strip()
                new_text = re.sub(r'\s+', ' ', new_text)
                return new_text, max(1, current_intensity - 1)
    
    return text, current_intensity


def weak_label_text(text: str) -> Tuple[Optional[str], int, float]:
    """
    Automatically label text using keyword matching (weak labeling).
    
    Args:
        text: Input text
        
    Returns:
        Tuple of (emotion, intensity, confidence)
        Returns (None, 0, 0) if no emotion detected
    """
    text_lower = text.lower()
    
    best_emotion = None
    best_intensity = 0
    best_confidence = 0.0
    
    for emotion, intensity_keywords in EMOTION_KEYWORDS.items():
        # Check strong keywords first
        for keyword in intensity_keywords["strong"]:
            if keyword in text_lower:
                if best_confidence < 0.9:
                    best_emotion = emotion
                    best_intensity = 5
                    best_confidence = 0.9
                break
        
        # Check medium keywords
        for keyword in intensity_keywords["medium"]:
            if keyword in text_lower:
                if best_confidence < 0.7:
                    best_emotion = emotion
                    best_intensity = 3
                    best_confidence = 0.7
                break
        
        # Check weak keywords
        for keyword in intensity_keywords["weak"]:
            if keyword in text_lower:
                if best_confidence < 0.5:
                    best_emotion = emotion
                    best_intensity = 2
                    best_confidence = 0.5
                break
    
    return best_emotion, best_intensity, best_confidence


def augment_dataset(
    samples: List[Dict],
    augmentation_factor: int = 2,
    include_weak_labeled: bool = True
) -> List[Dict]:
    """
    Augment dataset with synonym replacement and intensity variations.
    
    Args:
        samples: Original samples
        augmentation_factor: How many augmented samples per original
        include_weak_labeled: Whether to include weak-labeled samples
        
    Returns:
        Augmented dataset
    """
    augmented = []
    
    for sample in samples:
        # Keep original
        sample['confidence'] = 1.0
        sample['augmentation_type'] = 'original'
        augmented.append(sample.copy())
        
        text = sample['text']
        emotion = sample['primary_emotion']
        intensity = sample['intensity']
        
        # Synonym replacement
        for i in range(augmentation_factor):
            aug_text = synonym_replacement(text, n_replacements=1)
            if aug_text != text:
                aug_sample = sample.copy()
                aug_sample['text'] = aug_text
                aug_sample['confidence'] = 0.95
                aug_sample['augmentation_type'] = 'synonym'
                augmented.append(aug_sample)
        
        # Intensity variation (only if intensity can change)
        if 2 <= intensity <= 4:
            var_text, var_intensity = intensity_variation(text, intensity)
            if var_text != text:
                var_sample = sample.copy()
                var_sample['text'] = var_text
                var_sample['intensity'] = var_intensity
                var_sample['confidence'] = 0.9
                var_sample['augmentation_type'] = 'intensity'
                augmented.append(var_sample)
    
    return augmented


def generate_weak_labeled_samples(
    seed_texts: List[str],
    emoji_map: Dict[str, List[str]]
) -> List[Dict]:
    """
    Generate weak-labeled samples from unlabeled texts.
    
    Args:
        seed_texts: List of unlabeled Vietnamese texts
        emoji_map: Emotion to emoji mapping
        
    Returns:
        List of weak-labeled samples
    """
    samples = []
    
    for text in seed_texts:
        emotion, intensity, confidence = weak_label_text(text)
        
        if emotion and confidence >= 0.5:
            emojis = emoji_map.get(emotion, ["ü§î", "üòä", "üëç"])
            
            sample = {
                'text': text,
                'primary_emotion': emotion,
                'intensity': intensity,
                'emoji_1': emojis[0] if len(emojis) > 0 else "ü§î",
                'emoji_2': emojis[1] if len(emojis) > 1 else None,
                'emoji_3': emojis[2] if len(emojis) > 2 else None,
                'confidence': confidence,
                'augmentation_type': 'weak_labeled',
                'source': 'auto'
            }
            samples.append(sample)
    
    return samples


def validate_dataset(samples: List[Dict], min_confidence: float = 0.5) -> Dict:
    """
    Validate dataset quality.
    
    Args:
        samples: Dataset samples
        min_confidence: Minimum confidence threshold
        
    Returns:
        Validation report
    """
    total = len(samples)
    high_quality = sum(1 for s in samples if s.get('confidence', 1.0) >= 0.9)
    medium_quality = sum(1 for s in samples if 0.7 <= s.get('confidence', 1.0) < 0.9)
    low_quality = sum(1 for s in samples if s.get('confidence', 1.0) < 0.7)
    
    # Emotion distribution
    emotion_counts = {}
    for s in samples:
        emotion = s.get('primary_emotion', 'unknown')
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
    
    # Check balance
    avg_per_emotion = total / len(emotion_counts) if emotion_counts else 0
    imbalance_ratio = max(emotion_counts.values()) / min(emotion_counts.values()) if emotion_counts else 0
    
    return {
        'total_samples': total,
        'high_quality': high_quality,
        'medium_quality': medium_quality,
        'low_quality': low_quality,
        'emotion_distribution': emotion_counts,
        'avg_per_emotion': avg_per_emotion,
        'imbalance_ratio': imbalance_ratio,
        'is_balanced': imbalance_ratio < 2.0,
        'quality_score': (high_quality * 1.0 + medium_quality * 0.7 + low_quality * 0.4) / total if total > 0 else 0
    }


# Sample unlabeled texts for weak labeling
SAMPLE_UNLABELED_TEXTS = [
    # Joy/Happy
    "H√¥m nay tr·ªùi ƒë·∫πp qu√°",
    "ƒê∆∞·ª£c tƒÉng l∆∞∆°ng r·ªìi",
    "Cu·ªëi tu·∫ßn ƒëi ch∆°i",
    "Team m√¨nh th·∫Øng r·ªìi",
    "ƒÇn ngon qu√°",
    "Phim hay gh√™",
    "ƒê∆∞·ª£c ngh·ªâ ph√©p vui qu√°",
    "G·∫∑p l·∫°i b·∫°n c≈©",
    "Sinh nh·∫≠t vui v·∫ª",
    "ƒê·∫°t ƒëi·ªÉm cao",
    "Tr√∫ng gi·∫£i r·ªìi",
    "Pass ph·ªèng v·∫•n",
    "ƒê∆∞·ª£c thƒÉng ch·ª©c",
    "C√≥ ng∆∞·ªùi y√™u r·ªìi",
    "Mua ƒë∆∞·ª£c nh√†",
    "Du l·ªãch th√¥i",
    "G·∫∑p th·∫ßn t∆∞·ª£ng",
    "Ng·ªß ngon l√†nh",
    "V·ªÅ qu√™ ƒÉn t·∫øt",
    "Ngh·ªâ h·ªçc h√¥m nay",
    "ƒê·ªì ƒÉn ngon tuy·ªát",
    "Tr·ªùi ∆°i qu√° may",
    "B·∫°n mua cho qu√†",
    "Th·∫Øng c∆∞·ª£c r·ªìi",
    "H·∫øt d·ªãch r·ªìi",
    "ƒê∆∞·ª£c ƒëi du h·ªçc",
    "Pass k·ª≥ thi",
    "Si√™u th√≠ch lu√¥n",
    
    # Sadness
    "C√¥ng vi·ªác √°p l·ª±c qu√°",
    "B·∫°n b√® xa d·∫ßn",
    "Th·ª©c khuya qu√° m·ªát",
    "B·ªã cancel k·∫ø ho·∫°ch",
    "H·ªçc kh√¥ng hi·ªÉu g√¨",
    "D·ªçn ph√≤ng m·ªát gh√™",
    "Th·∫•t t√¨nh r·ªìi",
    "B·ªã ƒëi·ªÉm k√©m",
    "M·∫•t ƒë·ªì r·ªìi",
    "Kh√¥ng c√≥ ti·ªÅn",
    "Xa nh√† qu√°",
    "Th·ªùi ti·∫øt x·∫•u",
    "H·∫øt pin r·ªìi",
    "Kh√¥ng ng·ªß ƒë∆∞·ª£c",
    "Qu√™n mang v√≠",
    "Tr·ªÖ xe bus",
    "·ªêm r·ªìi",
    "C·∫£m th·∫•y c√¥ ƒë∆°n",
    "Nh·ªõ ng∆∞·ªùi y√™u c≈©",
    "Th·∫•t nghi·ªáp r·ªìi",
    "Ph·∫£i ·ªü nh√†",
    "L·ª° chuy·∫øn bay",
    "Th·∫•t b·∫°i r·ªìi",
    "Kh√¥ng ai hi·ªÉu",
    
    # Anger
    "B·ªã s·∫øp m·∫Øng",
    "Deadline g·∫•p qu√°",
    "ƒê∆∞·ªùng t·∫Øc kinh kh·ªßng",
    "B·ªã l·ª´a r·ªìi",
    "D·ªãch v·ª• t·ªá qu√°",
    "M·∫°ng ch·∫≠m gh√™",
    "B·ªã c∆∞·ªõp m·∫•t",
    "B·ªã n√≥i x·∫•u",
    "Kh√¥ng c√¥ng b·∫±ng",
    "T·ª©c m√¨nh qu√°",
    "ƒê·ª£i l√¢u gh√™",
    "B·ªã ph·∫°t ti·ªÅn",
    "L√†m sai quy tr√¨nh",
    "B·ªã t·ª´ ch·ªëi",
    "Ch·∫≠m tr·ªÖ ho√†i",
    "B·ªã h·ªßy ƒë∆°n",
    "D·ªãch v·ª• k√©m",
    "Kh√¥ng tr·∫£ l·ªùi",
    
    # Fear/Anxiety
    "Mai thi r·ªìi lo qu√°",
    "Ch·ªù k·∫øt qu·∫£ h·ªìi h·ªôp",
    "S·∫Øp h·∫øt h·∫°n",
    "Kh√¥ng k·ªãp r·ªìi",
    "Qu√™n l√†m b√†i t·∫≠p",
    "B·ªë m·∫π bi·∫øt m·∫•t",
    "S·∫Øp h·∫øt ti·ªÅn",
    "Kh√¥ng chu·∫©n b·ªã",
    "Qu√° h·∫°n r·ªìi",
    "Ph·∫£i h·ªçp g·∫•p",
    "G·∫∑p √¥ng ch·ªß",
    "Ki·ªÉm tra ƒë·ªôt xu·∫•t",
    "Nguy hi·ªÉm qu√°",
    "Kh√¥ng bi·∫øt l√†m sao",
    "Lo l·∫Øng m√£i",
    "B·ªë m·∫π gi·∫≠n r·ªìi",
    
    # Surprise
    "Tin ƒë∆∞·ª£c kh√¥ng",
    "Kh√¥ng ng·ªù v·∫≠y",
    "Th·∫≠t √†",
    "Sao l·∫°i th·∫ø",
    "Tr·ªùi ∆°i ƒë·∫•t h·ª°i",
    "B·∫•t ng·ªù gh√™",
    "Gi√° r·∫ª v·∫≠y",
    "Nhanh v·∫≠y sao",
    "Ai ng·ªù ƒë∆∞·ª£c",
    "Kh√°c h·∫≥n",
    "L·∫° th·∫≠t",
    "Kh√¥ng tin n·ªïi",
    "Th·∫≠t s·ª± √†",
    "·ª¶a v·∫≠y sao",
    
    # Disgust
    "D∆° b·∫©n qu√°",
    "Kh√≥ ch·ªãu gh√™",
    "H·∫øt ch·ªó r·ªìi",
    "H·ªèng r·ªìi",
    "Ch·∫•t l∆∞·ª£ng t·ªá",
    "Kh√¥ng s·∫°ch s·∫Ω",
    "M√πi kh√≥ ch·ªãu",
    "Tr√¥ng kinh kh·ªßng",
    "Kh√¥ng th√≠ch t√≠ n√†o",
    "T·ªá h·∫°i th·∫≠t",
    "Ch√°n ng·∫Øt",
    "Kh√¥ng ∆∞ng",
    
    # Trust/Support
    "Tin b·∫°n m√†",
    "C·ªë g·∫Øng l√™n",
    "B·∫°n l√†m ƒë∆∞·ª£c",
    "·ªîn m√†",
    "Tin t∆∞·ªüng nh√©",
    "ƒê·ª´ng lo",
    "Kh√¥ng sao ƒë√¢u",
    "H·ªó tr·ª£ nh√©",
    "C√πng nhau l√†m",
    "Gi√∫p ƒë·ª° nha",
    
    # Anticipation
    "S·∫Øp ƒë∆∞·ª£c ngh·ªâ",
    "Ch·ªù ƒë·ª£i m√£i",
    "Mong t·ªõi th·ª© 6",
    "S·∫Øp c√≥ k·ª≥ ngh·ªâ",
    "Ch·ªù m√£i",
    "S·∫Øp ƒë·∫øn r·ªìi",
    "C√≤n v√†i ng√†y",
    "Mong ƒë·ª£i qu√°",
    "H√°o h·ª©c gh√™",
    "Ch·ªù kh√¥ng n·ªïi",
]


if __name__ == "__main__":
    from data.collect_data import create_initial_dataset, save_dataset_csv
    from src.models import EMOTION_EMOJI_MAP
    
    print("=" * 60)
    print("DATA AUGMENTATION & WEAK LABELING")
    print("=" * 60)
    
    # 1. Load original dataset
    original_samples = create_initial_dataset()
    print(f"\n1. Original samples: {len(original_samples)}")
    
    # 2. Augment with synonyms
    augmented_samples = augment_dataset(original_samples, augmentation_factor=2)
    print(f"2. After augmentation: {len(augmented_samples)}")
    
    # 3. Generate weak-labeled samples
    weak_labeled = generate_weak_labeled_samples(SAMPLE_UNLABELED_TEXTS, EMOTION_EMOJI_MAP)
    print(f"3. Weak-labeled samples: {len(weak_labeled)}")
    
    # 4. Combine
    all_samples = augmented_samples + weak_labeled
    print(f"4. Total samples: {len(all_samples)}")
    
    # 5. Validate
    validation = validate_dataset(all_samples)
    print(f"\n5. Validation Report:")
    print(f"   - High quality: {validation['high_quality']}")
    print(f"   - Medium quality: {validation['medium_quality']}")
    print(f"   - Low quality: {validation['low_quality']}")
    print(f"   - Quality score: {validation['quality_score']:.2%}")
    print(f"   - Balanced: {validation['is_balanced']}")
    
    # 6. Save
    save_dataset_csv(all_samples, "data/processed/augmented_data.csv")
    print(f"\n‚úì Saved to data/processed/augmented_data.csv")
