"""
Text preprocessing module for Vietnamese Emoji Suggestion System.

This module handles:
- Teencode/slang normalization
- URL and special character removal
- Text cleaning and normalization
- Word segmentation (optional)
"""

import re
from typing import Optional, Dict, List
import unicodedata


# Comprehensive teencode dictionary for Vietnamese
TEENCODE_MAP = {
    # Common abbreviations
    "cháº±n zn": "tráº§n trá»¥i",
    "xá»‰u": "ngáº¥t",
    "sáº£ng": "Ä‘au Ä‘áº§u",
    "gáº¯t": "kháº¯t khe",
    "chÃ¡n chÃª": "chÃ¡n ngÃ¡n",
    
    # Common slang
    "ko": "khÃ´ng",
    "k": "khÃ´ng",
    "kh": "khÃ´ng",
    "khg": "khÃ´ng",
    "hok": "khÃ´ng",
    "hem": "khÃ´ng",
    "hÃ´ng": "khÃ´ng",
    
    "dc": "Ä‘Æ°á»£c",
    "Ä‘c": "Ä‘Æ°á»£c",
    "dk": "Ä‘Æ°á»£c",
    "Ä‘k": "Ä‘Æ°á»£c",
    "duoc": "Ä‘Æ°á»£c",
    
    "bn": "báº¡n",
    "báº¡n": "báº¡n",
    
    "mk": "mÃ¬nh",
    "mik": "mÃ¬nh",
    "mÃ¬k": "mÃ¬nh",
    
    "ng": "ngÆ°á»i",
    "ngta": "ngÆ°á»i ta",
    "ns": "nÃ³i",
    
    "r": "rá»“i",
    "oy": "rá»“i",
    "rui": "rá»“i",
    
    "trc": "trÆ°á»›c",
    "truoc": "trÆ°á»›c",
    "trc khi": "trÆ°á»›c khi",
    
    "sao Ä‘Ã³": "sau Ä‘Ã³",
    "sd": "sau Ä‘Ã³",
    
    "bt": "biáº¿t",
    "bÃ­t": "biáº¿t",
    "biet": "biáº¿t",
    "bjt": "biáº¿t",
    
    "nte": "nhÆ° tháº¿",
    "ntn": "nhÆ° tháº¿ nÃ o",
    "sao": "sao",
    
    "cx": "cÅ©ng",
    "cg": "cÅ©ng",
    "cug": "cÅ©ng",
    
    "vs": "vá»›i",
    "voi": "vá»›i",
    "vk": "vá»£",
    "ck": "chá»“ng",
    
    "thik": "thÃ­ch",
    "thix": "thÃ­ch",
    "thick": "thÃ­ch",
    
    "iu": "yÃªu",
    "iu qua": "yÃªu quÃ¡",
    "yeu": "yÃªu",
    
    "bh": "bao giá»",
    "bjh": "bao giá»",
    
    "lm": "lÃ m",
    "lam": "lÃ m",
    
    "nc": "nÆ°á»›c",
    "nuoc": "nÆ°á»›c",
    
    "nh": "nhiá»u",
    "nhiu": "nhiá»u",
    
    "qa": "quÃ¡",
    "qua": "quÃ¡",
    
    "thi": "thÃ¬",
    "thi": "thÃ¬",
    
    "Ä‘ag": "Ä‘ang",
    "dang": "Ä‘ang",
    "dag": "Ä‘ang",
    
    "hÄ‘": "hoáº¡t Ä‘á»™ng",
    "hd": "hoáº¡t Ä‘á»™ng",
    
    "z": "váº­y",
    "v": "váº­y",
    "vay": "váº­y",
    
    "nyc": "ngÆ°á»i yÃªu cÅ©",
    "ny": "ngÆ°á»i yÃªu",
    
    "sg": "SÃ i GÃ²n",
    "hn": "HÃ  Ná»™i",
    
    "ah": "Ã ",
    "ak": "áº¡",
    "a": "anh",
    "e": "em",
    
    "t": "tao",
    "m": "mÃ y",
    
    "vl": "vÃ£i",
    "vcl": "vÃ£i",
    "vll": "vÃ£i",
    
    "cmn": "con máº¹ nÃ³",
    "dm": "Ä‘Ã¹ mÃ¡",
    
    "gato": "ghen Äƒn tá»©c á»Ÿ",
    
    "ok": "Ä‘Æ°á»£c",
    "okie": "Ä‘Æ°á»£c",
    "oke": "Ä‘Æ°á»£c",
    "okla": "Ä‘Æ°á»£c",
    
    "bye": "táº¡m biá»‡t",
    "bai": "táº¡m biá»‡t",
    "bye bye": "táº¡m biá»‡t",
    
    "hi": "xin chÃ o",
    "hello": "xin chÃ o",
    "hellu": "xin chÃ o",
    
    "thks": "cáº£m Æ¡n",
    "thanks": "cáº£m Æ¡n",
    "tks": "cáº£m Æ¡n",
    "thank you": "cáº£m Æ¡n",
    
    "sorry": "xin lá»—i",
    "sr": "xin lá»—i",
    "sry": "xin lá»—i",
    
    "plz": "lÃ m Æ¡n",
    "pls": "lÃ m Æ¡n",
    "please": "lÃ m Æ¡n",
    
    "lol": "haha",
    "hehe": "haha",
    "hihi": "haha",
    "kk": "haha",
    "huhu": "buá»“n",
    
    "gÃ¬ z": "gÃ¬ váº­y",
    "gi z": "gÃ¬ váº­y",
    "j z": "gÃ¬ váº­y",
    "j v": "gÃ¬ váº­y",
    
    "Ä‘áº¹p zai": "Ä‘áº¹p trai",
    "dep zai": "Ä‘áº¹p trai",
    "Ä‘áº¹p gÃ¡i": "xinh gÃ¡i",
    
    "real": "tháº­t",
    "fake": "giáº£",
    
    "pro": "giá»i",
    "noob": "gÃ ",
    
    "hot": "nÃ³ng bá»ng",
    "cool": "tuyá»‡t",
    "cute": "dá»… thÆ°Æ¡ng",
    
    "wtf": "cÃ¡i gÃ¬",
    "omg": "trá»i Æ¡i",
    
    "sáº¿n": "sáº¿n",
    "ngáº§u": "ngáº§u",
    "cháº¥t": "cháº¥t",
    "xá»‹n": "xá»‹n",
    "max": "tá»‘i Ä‘a",
}

# Emotion-related keywords for reference
EMOTION_KEYWORDS = {
    "joy": ["vui", "háº¡nh phÃºc", "sung sÆ°á»›ng", "tuyá»‡t vá»i", "tá»‘t", "hay", 
            "chÃºc má»«ng", "yÃªu", "thÃ­ch", "cáº£m Æ¡n", "giá»i", "xuáº¥t sáº¯c"],
    "sadness": ["buá»“n", "Ä‘au", "khá»•", "tháº¥t vá»ng", "nhá»›", "cÃ´ Ä‘Æ¡n", 
                "chÃ¡n", "má»‡t", "thÆ°Æ¡ng", "tiáº¿c", "tá»™i nghiá»‡p"],
    "anger": ["giáº­n", "tá»©c", "bá»±c", "khÃ³ chá»‹u", "ghÃ©t", "á»©c", 
              "Ä‘iÃªn", "sá»‘t ruá»™t", "chÃ¡n", "quÃ¡ Ä‘Ã¡ng"],
    "fear": ["sá»£", "lo", "hoang mang", "cÄƒng tháº³ng", "run", "há»“i há»™p",
             "kinh", "Ä‘Ã¡ng sá»£", "rá»£n"],
    "surprise": ["ngáº¡c nhiÃªn", "báº¥t ngá»", "sá»‘c", "khÃ´ng ngá»", "wow", 
                 "á»§a", "trá»i Æ¡i", "tháº­t sao"],
    "disgust": ["ghÃª", "kinh", "tá»Ÿm", "dÆ¡", "báº©n", "ghÃ©t", "chÃ¡n",
                "ká»³", "dá»Ÿ", "tá»‡"],
    "trust": ["tin", "á»§ng há»™", "yÃªn tÃ¢m", "cháº¯c cháº¯n", "Ä‘Ã¡ng tin",
              "giá»i", "tá»‘t", "Ä‘Æ°á»£c"],
    "anticipation": ["mong", "chá»", "hÃ¡o há»©c", "hy vá»ng", "sáº¯p",
                     "cÃ²n", "Ä‘á»£i", "nÃ³ng lÃ²ng"]
}


def normalize_unicode(text: str) -> str:
    """Normalize Unicode characters to NFC form."""
    return unicodedata.normalize('NFC', text)


def remove_urls(text: str) -> str:
    """Remove URLs from text."""
    url_pattern = r'https?://\S+|www\.\S+'
    return re.sub(url_pattern, '', text)


def remove_emails(text: str) -> str:
    """Remove email addresses from text."""
    email_pattern = r'\S+@\S+\.\S+'
    return re.sub(email_pattern, '', text)


def remove_mentions(text: str) -> str:
    """Remove @mentions from text."""
    return re.sub(r'@\w+', '', text)


def remove_hashtags(text: str) -> str:
    """Remove #hashtags from text."""
    return re.sub(r'#\w+', '', text)


def remove_extra_whitespace(text: str) -> str:
    """Remove extra whitespace and normalize spaces."""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def remove_punctuation_except_basic(text: str) -> str:
    """Remove punctuation except basic sentence-ending marks."""
    # Keep: . , ! ? and Vietnamese diacritics
    text = re.sub(r'[^\w\s.,!?\u00C0-\u024F\u1E00-\u1EFF]', '', text)
    return text


def normalize_repeated_chars(text: str) -> str:
    """Normalize repeated characters (e.g., 'vuiiiii' -> 'vui')."""
    # Reduce 3+ repeated chars to 2
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    return text


def replace_teencode(text: str, teencode_dict: Optional[Dict[str, str]] = None) -> str:
    """Replace teencode/slang with standard Vietnamese."""
    if teencode_dict is None:
        teencode_dict = TEENCODE_MAP
    
    # Sort by length (longest first) to avoid partial replacements
    sorted_teencode = sorted(teencode_dict.items(), key=lambda x: len(x[0]), reverse=True)
    
    for slang, formal in sorted_teencode:
        # Case-insensitive replacement with word boundaries
        pattern = r'\b' + re.escape(slang) + r'\b'
        text = re.sub(pattern, formal, text, flags=re.IGNORECASE)
    
    return text


def extract_emojis(text: str) -> List[str]:
    """Extract all emoji characters from text."""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        "\U0001FA00-\U0001FA6F"  # Chess Symbols
        "\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        "]+", 
        flags=re.UNICODE
    )
    return emoji_pattern.findall(text)


def remove_emojis(text: str) -> str:
    """Remove all emoji characters from text."""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "\U0001F900-\U0001F9FF"
        "\U0001FA00-\U0001FA6F"
        "\U0001FA70-\U0001FAFF"
        "]+", 
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text)


def preprocess_text(
    text: str,
    lowercase: bool = True,
    remove_url: bool = True,
    remove_email: bool = True,
    remove_mention: bool = True,
    remove_hashtag: bool = True,
    normalize_teencode: bool = True,
    normalize_unicode_chars: bool = True,
    normalize_repeated: bool = True,
    remove_emoji: bool = True,
    custom_teencode_dict: Optional[Dict[str, str]] = None
) -> str:
    """
    Full preprocessing pipeline for Vietnamese text.
    
    Args:
        text: Input text to preprocess
        lowercase: Convert to lowercase
        remove_url: Remove URLs
        remove_email: Remove email addresses
        remove_mention: Remove @mentions
        remove_hashtag: Remove #hashtags
        normalize_teencode: Replace slang/teencode
        normalize_unicode_chars: Normalize Unicode to NFC
        normalize_repeated: Reduce repeated characters
        remove_emoji: Remove emoji characters
        custom_teencode_dict: Custom teencode mapping
    
    Returns:
        Preprocessed text
    """
    if not text:
        return ""
    
    # Unicode normalization first
    if normalize_unicode_chars:
        text = normalize_unicode(text)
    
    # Lowercase
    if lowercase:
        text = text.lower()
    
    # Remove URLs
    if remove_url:
        text = remove_urls(text)
    
    # Remove emails
    if remove_email:
        text = remove_emails(text)
    
    # Remove mentions
    if remove_mention:
        text = remove_mentions(text)
    
    # Remove hashtags
    if remove_hashtag:
        text = remove_hashtags(text)
    
    # Remove emojis
    if remove_emoji:
        text = remove_emojis(text)
    
    # Normalize repeated characters
    if normalize_repeated:
        text = normalize_repeated_chars(text)
    
    # Replace teencode
    if normalize_teencode:
        text = replace_teencode(text, custom_teencode_dict)
    
    # Clean up whitespace
    text = remove_extra_whitespace(text)
    
    return text


class TextPreprocessor:
    """
    Text preprocessor class with configurable options.
    """
    
    def __init__(
        self,
        lowercase: bool = True,
        remove_url: bool = True,
        remove_email: bool = True,
        remove_mention: bool = True,
        remove_hashtag: bool = True,
        normalize_teencode: bool = True,
        normalize_unicode_chars: bool = True,
        normalize_repeated: bool = True,
        remove_emoji: bool = True,
        custom_teencode_dict: Optional[Dict[str, str]] = None
    ):
        self.lowercase = lowercase
        self.remove_url = remove_url
        self.remove_email = remove_email
        self.remove_mention = remove_mention
        self.remove_hashtag = remove_hashtag
        self.normalize_teencode = normalize_teencode
        self.normalize_unicode_chars = normalize_unicode_chars
        self.normalize_repeated = normalize_repeated
        self.remove_emoji = remove_emoji
        self.teencode_dict = custom_teencode_dict or TEENCODE_MAP
    
    def preprocess(self, text: str) -> str:
        """Preprocess a single text."""
        return preprocess_text(
            text,
            lowercase=self.lowercase,
            remove_url=self.remove_url,
            remove_email=self.remove_email,
            remove_mention=self.remove_mention,
            remove_hashtag=self.remove_hashtag,
            normalize_teencode=self.normalize_teencode,
            normalize_unicode_chars=self.normalize_unicode_chars,
            normalize_repeated=self.normalize_repeated,
            remove_emoji=self.remove_emoji,
            custom_teencode_dict=self.teencode_dict
        )
    
    def preprocess_batch(self, texts: List[str]) -> List[str]:
        """Preprocess multiple texts."""
        return [self.preprocess(text) for text in texts]


if __name__ == "__main__":
    # Test preprocessing
    test_cases = [
        "ChÃºc má»«ng báº¡n! ğŸ‰ğŸŠ",
        "Ko bÃ­t sao lun huhu ğŸ˜¢",
        "Check out https://example.com @friend #happy",
        "Vui qaaaaaa!!! ğŸ˜ŠğŸ˜ŠğŸ˜Š",
        "Thik Ä‘c iu qaaa â¤ï¸â¤ï¸â¤ï¸",
        "Buá»“n quÃ¡ Ä‘i máº¥t thui ğŸ˜­",
    ]
    
    print("=== PREPROCESSING TESTS ===\n")
    
    preprocessor = TextPreprocessor()
    
    for text in test_cases:
        processed = preprocessor.preprocess(text)
        emojis = extract_emojis(text)
        print(f"Original: {text}")
        print(f"Processed: {processed}")
        print(f"Emojis extracted: {emojis}")
        print("-" * 50)
