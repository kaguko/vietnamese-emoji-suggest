"""
Data collection utilities for Vietnamese Emoji Suggestion System.

This module provides tools for collecting, organizing, and validating
training data for the emoji suggestion model.
"""

import json
import csv
import os
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict


# Emotion labels based on Plutchik's wheel of emotions
EMOTION_LABELS = {
    0: "joy",           # vui
    1: "sadness",       # buá»“n
    2: "anger",         # giáº­n
    3: "fear",          # sá»£
    4: "surprise",      # ngáº¡c nhiÃªn
    5: "disgust",       # ghÃª tá»Ÿm
    6: "trust",         # tin tÆ°á»Ÿng
    7: "anticipation"   # mong Ä‘á»£i
}

EMOTION_TO_IDX = {v: k for k, v in EMOTION_LABELS.items()}

# Intensity levels
INTENSITY_LEVELS = {
    1: "very_weak",     # gáº§n nhÆ° neutral
    2: "weak",          # yáº¿u
    3: "medium",        # trung bÃ¬nh
    4: "strong",        # máº¡nh
    5: "very_strong"    # ráº¥t máº¡nh
}


@dataclass
class DataSample:
    """A single data sample for training."""
    text: str
    primary_emotion: str
    intensity: int
    emoji_1: str
    emoji_2: Optional[str] = None
    emoji_3: Optional[str] = None
    source: str = "manual"
    created_at: str = ""
    
    def __post_init__(self):
        if not self.created_at:
            self.created_at = datetime.now().isoformat()
        
        # Validate emotion
        if self.primary_emotion not in EMOTION_TO_IDX:
            raise ValueError(f"Invalid emotion: {self.primary_emotion}. "
                           f"Must be one of {list(EMOTION_TO_IDX.keys())}")
        
        # Validate intensity
        if self.intensity not in INTENSITY_LEVELS:
            raise ValueError(f"Invalid intensity: {self.intensity}. Must be 1-5")


def create_initial_dataset() -> List[Dict]:
    """
    Create initial manually-curated dataset.
    This provides 100+ samples covering all 8 emotions.
    """
    samples = [
        # JOY (vui)
        DataSample("ChÃºc má»«ng báº¡n Ä‘áº­u tuyá»ƒn dá»¥ng!", "joy", 5, "ðŸ˜Š", "ðŸŽ‰", "ðŸ¥³"),
        DataSample("HÃ´m nay lÃ  ngÃ y tuyá»‡t vá»i!", "joy", 4, "ðŸ˜„", "âœ¨", "ðŸŒŸ"),
        DataSample("Cáº£m Æ¡n báº¡n nhiá»u láº¯m!", "joy", 4, "ðŸ˜Š", "ðŸ™", "â¤ï¸"),
        DataSample("MÃ¬nh vui quÃ¡!", "joy", 5, "ðŸ˜", "ðŸŽŠ", "ðŸ’«"),
        DataSample("Tháº­t tuyá»‡t vá»i!", "joy", 4, "ðŸ¤©", "ðŸ‘", "âœ¨"),
        DataSample("Cuá»‘i cÃ¹ng cÅ©ng xong!", "joy", 4, "ðŸ˜Œ", "ðŸŽ‰", "ðŸ’ª"),
        DataSample("ÄÆ°á»£c nghá»‰ phÃ©p rá»“i!", "joy", 4, "ðŸ˜Š", "ðŸ–ï¸", "ðŸŽŠ"),
        DataSample("LÆ°Æ¡ng thÃ¡ng nÃ y tÄƒng!", "joy", 5, "ðŸ¤‘", "ðŸ’°", "ðŸŽ‰"),
        DataSample("Con Ä‘á»— Ä‘áº¡i há»c rá»“i máº¹ Æ¡i!", "joy", 5, "ðŸ˜­", "ðŸŽ“", "ðŸŽ‰"),
        DataSample("Ä‚n má»«ng thÃ´i!", "joy", 4, "ðŸŽ‰", "ðŸ»", "ðŸ¥³"),
        DataSample("Vui ghÃª!", "joy", 3, "ðŸ˜Š", "ðŸ˜„", "ðŸ™‚"),
        DataSample("CÃ³ tin vui nÃ¨!", "joy", 4, "ðŸ˜Š", "âœ¨", "ðŸŒˆ"),
        
        # SADNESS (buá»“n)
        DataSample("Buá»“n quÃ¡", "sadness", 5, "ðŸ˜­", "ðŸ˜¢", "ðŸ’”"),
        DataSample("MÃ¬nh ráº¥t nhá»› báº¡n", "sadness", 4, "ðŸ˜¢", "ðŸ’”", "ðŸ¥º"),
        DataSample("Tháº­t Ä‘Ã¡ng tiáº¿c", "sadness", 3, "ðŸ˜”", "ðŸ˜ž", "ðŸ’”"),
        DataSample("HÃ´m nay khÃ´ng vui", "sadness", 3, "ðŸ˜”", "â˜¹ï¸", "ðŸ˜ž"),
        DataSample("Thi trÆ°á»£t rá»“i", "sadness", 4, "ðŸ˜­", "ðŸ˜¢", "ðŸ’”"),
        DataSample("Chia tay rá»“i", "sadness", 5, "ðŸ’”", "ðŸ˜¢", "ðŸ˜­"),
        DataSample("Máº¥t viá»‡c rá»“i", "sadness", 5, "ðŸ˜­", "ðŸ˜ž", "ðŸ’”"),
        DataSample("CÃ´ Ä‘Æ¡n quÃ¡", "sadness", 4, "ðŸ˜¢", "ðŸ¥º", "ðŸ˜”"),
        DataSample("Nhá»› nhÃ ", "sadness", 4, "ðŸ¥º", "ðŸ˜¢", "ðŸ "),
        DataSample("Tháº¥t vá»ng quÃ¡", "sadness", 4, "ðŸ˜ž", "ðŸ˜”", "ðŸ’”"),
        DataSample("ChÃ¡n ghÃª", "sadness", 3, "ðŸ˜•", "ðŸ˜”", "ðŸ˜’"),
        DataSample("Má»‡t má»i láº¯m", "sadness", 3, "ðŸ˜©", "ðŸ˜ž", "ðŸ˜”"),
        
        # ANGER (giáº­n)
        DataSample("Tá»©c quÃ¡!", "anger", 5, "ðŸ˜ ", "ðŸ’¢", "ðŸ˜¤"),
        DataSample("Sao láº¡i tháº¿ Ä‘Æ°á»£c!", "anger", 4, "ðŸ˜¡", "ðŸ’¢", "ðŸ˜¤"),
        DataSample("Bá»±c mÃ¬nh ghÃª!", "anger", 4, "ðŸ˜¤", "ðŸ’¢", "ðŸ˜ "),
        DataSample("KhÃ´ng cháº¥p nháº­n Ä‘Æ°á»£c!", "anger", 5, "ðŸ˜¡", "ðŸ‘Š", "ðŸ’¢"),
        DataSample("QuÃ¡ Ä‘Ã¡ng!", "anger", 4, "ðŸ˜ ", "ðŸ’¢", "ðŸ¤¬"),
        DataSample("GhÃ©t cay ghÃ©t Ä‘áº¯ng!", "anger", 5, "ðŸ¤¬", "ðŸ’¢", "ðŸ˜¡"),
        DataSample("ÄiÃªn tiáº¿t lÃªn Ä‘Æ°á»£c!", "anger", 5, "ðŸ¤¬", "ðŸ’¢", "ðŸ˜¤"),
        DataSample("MÃ y lÃ m gÃ¬ váº­y!", "anger", 4, "ðŸ˜ ", "ðŸ’¢", "ðŸ˜¤"),
        DataSample("KhÃ³ chá»‹u quÃ¡!", "anger", 3, "ðŸ˜’", "ðŸ˜¤", "ðŸ’¢"),
        DataSample("Sá»‘t ruá»™t quÃ¡!", "anger", 3, "ðŸ˜¤", "â°", "ðŸ˜ "),
        
        # FEAR (sá»£)
        DataSample("Sá»£ quÃ¡!", "fear", 5, "ðŸ˜¨", "ðŸ˜±", "ðŸ˜°"),
        DataSample("Lo láº¯ng quÃ¡!", "fear", 4, "ðŸ˜°", "ðŸ˜Ÿ", "ðŸ¥º"),
        DataSample("CÄƒng tháº³ng quÃ¡!", "fear", 4, "ðŸ˜°", "ðŸ˜¬", "ðŸ’¦"),
        DataSample("KhÃ´ng dÃ¡m Ä‘Ã¢u!", "fear", 3, "ðŸ˜¨", "ðŸ™ˆ", "ðŸ˜°"),
        DataSample("Run háº¿t cáº£ ngÆ°á»i!", "fear", 5, "ðŸ˜±", "ðŸ˜¨", "ðŸ’€"),
        DataSample("Há»“i há»™p quÃ¡!", "fear", 3, "ðŸ˜¬", "ðŸ’“", "ðŸ˜°"),
        DataSample("Mai thi rá»“i!", "fear", 4, "ðŸ˜°", "ðŸ“š", "ðŸ˜±"),
        DataSample("ÄÃ¡ng sá»£ tháº­t!", "fear", 4, "ðŸ˜±", "ðŸ˜¨", "ðŸ‘»"),
        DataSample("Hoang mang quÃ¡!", "fear", 4, "ðŸ˜°", "ðŸ˜Ÿ", "â“"),
        DataSample("KhÃ´ng biáº¿t sao!", "fear", 3, "ðŸ˜°", "ðŸ¤”", "ðŸ˜Ÿ"),
        
        # SURPRISE (ngáº¡c nhiÃªn)
        DataSample("Tháº­t sao!", "surprise", 4, "ðŸ˜®", "ðŸ˜²", "ðŸ¤¯"),
        DataSample("KhÃ´ng tin Ä‘Æ°á»£c!", "surprise", 5, "ðŸ˜±", "ðŸ¤¯", "ðŸ˜®"),
        DataSample("Æ  kÃ¬a!", "surprise", 3, "ðŸ˜®", "â“", "ðŸ˜¯"),
        DataSample("Báº¥t ngá» quÃ¡!", "surprise", 5, "ðŸ¤¯", "ðŸ˜²", "ðŸŽ‰"),
        DataSample("Wow!", "surprise", 4, "ðŸ¤©", "ðŸ˜®", "âœ¨"),
        DataSample("Trá»i Æ¡i!", "surprise", 4, "ðŸ˜±", "ðŸ˜®", "ðŸ™€"),
        DataSample("KhÃ´ng ngá»!", "surprise", 4, "ðŸ˜²", "ðŸ˜®", "ðŸ¤¯"),
        DataSample("á»¦a!", "surprise", 3, "ðŸ¤”", "ðŸ˜®", "â“"),
        DataSample("GÃ¬ Ä‘Ã¢y!", "surprise", 3, "ðŸ˜®", "ðŸ¤”", "â“"),
        DataSample("ÄÃ¹a Ã !", "surprise", 4, "ðŸ˜²", "ðŸ¤£", "ðŸ˜®"),
        
        # DISGUST (ghÃª tá»Ÿm)
        DataSample("GhÃª quÃ¡!", "disgust", 5, "ðŸ¤¢", "ðŸ¤®", "ðŸ˜–"),
        DataSample("Kinh dá»‹!", "disgust", 4, "ðŸ˜±", "ðŸ¤¢", "ðŸ‘Ž"),
        DataSample("KhÃ´ng chá»‹u Ä‘Æ°á»£c!", "disgust", 4, "ðŸ¤®", "ðŸ˜–", "âŒ"),
        DataSample("DÆ¡ báº©n quÃ¡!", "disgust", 4, "ðŸ¤¢", "ðŸ˜·", "ðŸš«"),
        DataSample("Ká»³ quÃ¡!", "disgust", 3, "ðŸ˜’", "ðŸ™„", "ðŸ˜•"),
        DataSample("á»šn láº¡nh!", "disgust", 4, "ðŸ˜–", "ðŸ¤¢", "ðŸ˜¬"),
        DataSample("KhÃ´ng thÃ­ch!", "disgust", 3, "ðŸ‘Ž", "ðŸ˜•", "âŒ"),
        DataSample("Tá»‡ quÃ¡!", "disgust", 4, "ðŸ‘Ž", "ðŸ˜¤", "ðŸ’”"),
        DataSample("Dá»Ÿ áº¹c!", "disgust", 3, "ðŸ‘Ž", "ðŸ˜’", "ðŸ™„"),
        DataSample("ChÃ¡n ngáº¥y!", "disgust", 4, "ðŸ˜’", "ðŸ™„", "ðŸ˜¤"),
        
        # TRUST (tin tÆ°á»Ÿng)
        DataSample("Tin báº¡n!", "trust", 4, "ðŸ¤", "ðŸ’ª", "ðŸ‘"),
        DataSample("Cáº­u lÃ m Ä‘Æ°á»£c!", "trust", 4, "ðŸ’ª", "âœ¨", "ðŸ‘"),
        DataSample("MÃ¬nh á»§ng há»™!", "trust", 4, "ðŸ‘", "ðŸ’ª", "ðŸ¤"),
        DataSample("YÃªn tÃ¢m Ä‘i!", "trust", 4, "ðŸ˜Œ", "ðŸ¤—", "ðŸ‘Œ"),
        DataSample("ÄÃ¡ng tin cáº­y!", "trust", 4, "ðŸ¤", "âœ…", "ðŸ’¯"),
        DataSample("CÃ¹ng nhau nhÃ©!", "trust", 4, "ðŸ¤", "ðŸ’ª", "â¤ï¸"),
        DataSample("KhÃ´ng lo!", "trust", 3, "ðŸ‘Œ", "ðŸ˜Š", "âœŒï¸"),
        DataSample("Cháº¯c cháº¯n!", "trust", 5, "âœ…", "ðŸ’¯", "ðŸ‘"),
        DataSample("Báº¡n giá»i láº¯m!", "trust", 4, "ðŸ‘", "ðŸŒŸ", "ðŸ’ª"),
        DataSample("Tuyá»‡t vá»i!", "trust", 4, "ðŸ‘", "âœ¨", "ðŸ”¥"),
        
        # ANTICIPATION (mong Ä‘á»£i)
        DataSample("Mong chá» quÃ¡!", "anticipation", 4, "ðŸ¤ž", "ðŸ˜Š", "âœ¨"),
        DataSample("HÃ¡o há»©c quÃ¡!", "anticipation", 5, "ðŸ¤©", "ðŸ˜†", "ðŸŽŠ"),
        DataSample("KhÃ´ng Ä‘á»£i Ä‘Æ°á»£c ná»¯a!", "anticipation", 5, "ðŸ˜†", "ðŸ”¥", "â°"),
        DataSample("Sáº¯p Ä‘áº¿n rá»“i!", "anticipation", 4, "ðŸŽ‰", "â°", "âœ¨"),
        DataSample("Chá» Ä‘á»£i!", "anticipation", 3, "â°", "ðŸ¤ž", "ðŸ˜Š"),
        DataSample("Hy vá»ng!", "anticipation", 4, "ðŸ¤ž", "âœ¨", "ðŸ™"),
        DataSample("Mai lÃ  sinh nháº­t!", "anticipation", 5, "ðŸŽ‚", "ðŸŽ‰", "ðŸ¤©"),
        DataSample("CÃ²n 3 ngÃ y ná»¯a!", "anticipation", 4, "â³", "ðŸ¤ž", "ðŸ˜Š"),
        DataSample("Cuá»‘i tuáº§n rá»“i!", "anticipation", 4, "ðŸŽ‰", "ðŸ¥³", "âœ¨"),
        DataSample("Sáº¯p nghá»‰ hÃ¨!", "anticipation", 5, "ðŸ–ï¸", "â˜€ï¸", "ðŸŽ‰"),
    ]
    
    return [asdict(s) for s in samples]


def save_dataset_csv(samples: List[Dict], filepath: str):
    """Save dataset to CSV file."""
    if not samples:
        print("No samples to save!")
        return
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    fieldnames = list(samples[0].keys())
    with open(filepath, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(samples)
    
    print(f"Saved {len(samples)} samples to {filepath}")


def save_dataset_json(samples: List[Dict], filepath: str):
    """Save dataset to JSON file."""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(samples, f, ensure_ascii=False, indent=2)
    
    print(f"Saved {len(samples)} samples to {filepath}")


def load_dataset_csv(filepath: str) -> List[Dict]:
    """Load dataset from CSV file."""
    samples = []
    with open(filepath, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            row['intensity'] = int(row['intensity'])
            samples.append(row)
    return samples


def load_dataset_json(filepath: str) -> List[Dict]:
    """Load dataset from JSON file."""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_dataset_stats(samples: List[Dict]) -> Dict:
    """Get statistics about the dataset."""
    stats = {
        'total_samples': len(samples),
        'emotions': {},
        'intensities': {},
        'avg_text_length': 0,
        'emoji_counts': {}
    }
    
    total_length = 0
    for sample in samples:
        # Count emotions
        emotion = sample['primary_emotion']
        stats['emotions'][emotion] = stats['emotions'].get(emotion, 0) + 1
        
        # Count intensities
        intensity = sample['intensity']
        stats['intensities'][intensity] = stats['intensities'].get(intensity, 0) + 1
        
        # Text length
        total_length += len(sample['text'].split())
        
        # Count emojis
        for key in ['emoji_1', 'emoji_2', 'emoji_3']:
            emoji = sample.get(key)
            if emoji:
                stats['emoji_counts'][emoji] = stats['emoji_counts'].get(emoji, 0) + 1
    
    stats['avg_text_length'] = total_length / len(samples) if samples else 0
    
    return stats


def validate_dataset(samples: List[Dict]) -> Dict:
    """Validate dataset for common issues."""
    issues = {
        'missing_emoji': [],
        'invalid_emotion': [],
        'invalid_intensity': [],
        'duplicate_text': [],
        'short_text': []
    }
    
    seen_texts = set()
    
    for i, sample in enumerate(samples):
        # Check for missing primary emoji
        if not sample.get('emoji_1'):
            issues['missing_emoji'].append(i)
        
        # Check emotion validity
        if sample.get('primary_emotion') not in EMOTION_TO_IDX:
            issues['invalid_emotion'].append(i)
        
        # Check intensity validity
        if sample.get('intensity') not in INTENSITY_LEVELS:
            issues['invalid_intensity'].append(i)
        
        # Check for duplicates
        text = sample.get('text', '').strip().lower()
        if text in seen_texts:
            issues['duplicate_text'].append(i)
        seen_texts.add(text)
        
        # Check for very short texts
        if len(text.split()) < 2:
            issues['short_text'].append(i)
    
    return issues


if __name__ == "__main__":
    # Create initial dataset
    print("Creating initial dataset...")
    samples = create_initial_dataset()
    
    # Get stats
    stats = get_dataset_stats(samples)
    print(f"\nDataset Statistics:")
    print(f"  Total samples: {stats['total_samples']}")
    print(f"  Emotions: {stats['emotions']}")
    print(f"  Avg text length: {stats['avg_text_length']:.1f} words")
    
    # Validate
    issues = validate_dataset(samples)
    total_issues = sum(len(v) for v in issues.values())
    if total_issues == 0:
        print("\nâœ“ Dataset validation passed!")
    else:
        print(f"\nâš  Found {total_issues} issues:")
        for issue_type, indices in issues.items():
            if indices:
                print(f"  {issue_type}: {len(indices)} samples")
    
    # Save to files
    save_dataset_csv(samples, "data/raw/initial_data.csv")
    save_dataset_json(samples, "data/raw/initial_data.json")
    
    print("\nâœ“ Initial dataset created successfully!")
